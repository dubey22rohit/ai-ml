{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# custom functions\n",
    "import utils\n",
    "\n",
    "# supress warnings\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input x: (50, 1)\n",
      "shape of targets y: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "# load dataset from text file\n",
    "data = np.loadtxt('./data/data_w3_ex1.csv', delimiter=',')\n",
    "\n",
    "# split the inputs and outputs in separate arrays\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "# convert 1D arrays to 2D as commands will require it later\n",
    "x = np.expand_dims(x, axis=1)\n",
    "y = np.expand_dims(y, axis=1)\n",
    "\n",
    "print(f\"shape of input x: {x.shape}\")\n",
    "print(f\"shape of targets y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (30, 1)\n",
      "the shape of the training set (target) is: (30, 1)\n",
      "\n",
      "the shape of the cross validation set (input) is: (10, 1)\n",
      "the shape of the cross validation set (target) is: (10, 1)\n",
      "\n",
      "the shape of the test set (input) is: (10, 1)\n",
      "the shape of the test set (target) is: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# splitting data in training, cross validation and test set\n",
    "# 60% - training, 20% - cross validation / dev set, 20% - test set\n",
    "\n",
    "# split 60% dataset as the training set. Put 40% in temp variables: x_ and y_\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# split 40% subset of the above into two: one half for dev set, other for test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# delete temp variables\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NNs can learn non-linear relationships so we can opt to skip polynomial features\n",
    "The code is still included below in case you want to try later and see what effect it will have on your results. The default `degree` is set to `1` to indicate that it will just use `x_train`, `x_cv`, and `x_test` as is (i.e. without any additional polynomial features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "degree = 1\n",
    "poly = PolynomialFeatures(degree, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will scale the input features to help gradient descent converge faster. Again, notice that you are using the mean and standard deviation computed from the training set by just using `transform()` in the cross validation and test sets instead of `fit_transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the features using z-score\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model_1\n",
      "Done!\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "training model_2\n",
      "Done!\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "training model_3\n",
      "Done!\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "RESULTS: \n",
      "Model 1: Training MSE: 406.19, CV MSE 551.78\n",
      "Model 2: Training MSE: 73.40, CV MSE 112.29\n",
      "Model 3: Training MSE: 406.19, CV MSE 551.78\n"
     ]
    }
   ],
   "source": [
    "# build and train models\n",
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "# Build the models\n",
    "nn_models = utils.build_models()\n",
    "\n",
    "for model in nn_models:\n",
    "\n",
    "    # setup the loss and optimizer\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "    )\n",
    "\n",
    "    print(f\"training {model.name}\")\n",
    "\n",
    "    # train the model\n",
    "    model.fit(\n",
    "        X_train_mapped_scaled, y_train,\n",
    "        epochs = 500,\n",
    "        verbose = 0\n",
    "    )\n",
    "\n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    # record the training MSEs\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "\n",
    "    # record the cross validation MSEs\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "# print results\n",
    "print(\"RESULTS: \")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num + 1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" + \n",
    "        f\"CV MSE {nn_cv_mses[model_num]:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Selected Model: 2\n",
      "Training MSE: 73.40\n",
      "Cross Validation MSE: 112.29\n",
      "Test MSE: 131.57\n"
     ]
    }
   ],
   "source": [
    "# select the model with lowest CV_MSE\n",
    "model_num = 2\n",
    "\n",
    "yhat = nn_models[model_num - 1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
